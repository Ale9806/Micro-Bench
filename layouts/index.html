{{ define "main" }}

<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<!-- <link rel="stylesheet" href="https://wilds.stanford.edu/doks-theme/assets/css/style.css"> -->

<section class="section hero-subheader container-fluid mt-n3 pb-3">

  <div class="container">
    <!-- First Row -->
      <div class="row justify-content-center">
            <div class="col-lg-12 text-center">
              <h1 class="mt-0">μ-Bench</h1>
            </div>

        <div class="col-lg-9 col-xl-8 text-center">
           <p class="lead">A Vision-Language Benchmark for Microscopy Understanding</p>
        </div>
      </div>

    <!-- Second  Row -->
      <div class="row justify-content-center">
          <div class="col-lg-9 col-xl-8 text-center">
            <a class="btn btn-primary btn-lg px-4 mb-2" href="https://arxiv.org/abs/2308.14089" role="button">Paper</a>
            <a class="btn btn-primary btn-lg px-4 mb-2" href="https://github.com/yeung-lab/u-Bench" role="button">GitHub</a>
          </div>
        </div>

        <section class="section section-sm">

            <div class="row justify-content-center text-center">
              <div class="col-lg-5">
                <h2 class="h4"><a href="https://github.com/yeung-lab/u-Bench/discussions/3">Contribute tasks</a></h2>
                <p> Help us go beyond 22 tasks.  <a href="https://github.com/yeung-lab/u-Bench/discussions/3">Contribute here</a></p>
                
              
              </div>
              <div class="col-lg-5">
                <h2 class="h4"><a href="https://github.com/yeung-lab/u-Bench/discussions">Contact us</a></h2>
                <p><a href="https://github.com/yeung-lab/u-Bench/discussions">Contact us</a> with questions and suggestions.
                </p>
              </div>
              <div class="col-lg-5">
                <h2 class="h4"><a href="docs/dataset/access">Dataset access</a></h2>
                <p>Coming soon.
                </p>
              </div>
            </div>
          </div>
        </section>

        <!-- Third  Row -->
        <div style="text-align: justify; text-justify: inter-word;">
            </p style="text-align: justify; text-justify: inter-word;">Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers’ efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs’ perception and cognition capabilities in biological image understanding. To address this gap, we introduce μ-Bench, an expert-curated benchmark encompassing 22 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular,tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on
μ-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data
often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release μ-Bench under a permissive license to accelerate the research and development of microscopy foundation models.</p>
           </div>

        <!-- Fourth Row -->
        <div class="row justify-content-center  d-md-block">
          <img src="https://raw.githubusercontent.com/Ale9806/uBench-website/main/images/UbenchFig1.png" alt="Rectangle"  class="border-0" >
              <div style="text-align: justify; text-justify: inter-word;">
            </p style="text-align: justify; text-justify: inter-word;"> <strong> Figure 1: </strong>strong> Data samples from μ-Bench, covering perception <div style="color: blue;">(left)</div> and cognition (right) tasks across
                subcellular, cellular, and tissue levels tasks across electron, fluorescence, and light microscopy</p>
        </div>


        <div class="row justify-content-center  d-md-block">
         
            </p style="text-align: justify; text-justify: inter-word;"> We developed a benchmark to assess the perception and cognition capabilities of VLMs in microscopy image analysis following the methodology shown in Figure 2. 
                                                                        At a high level, the pipeline consists of two main components:  (i) An biomedical expert categorized potential tasks and collected diverse microscopy datasets across multiple scientific domains, focusing on evaluating perception capabilities. 
                                                                        (ii) We then complement μ-Bench by crowdsourcing questions from a larger group of microscopists using a web application.</p>

           <img src="https://raw.githubusercontent.com/Ale9806/uBench-website/main/images/generation.png" alt="Rectangle"  class="border-0" >

          </p style="text-align: justify; text-justify: inter-word;">   Figure 2: μ-Bench construction protocol. Perception dataset (Left): first taxonomize use cases
across subcellular, cellular, and tissue-level applications and collect representative datasets spanning
multiple imaging modalities to test those scenarios. Next, datasets are converted to a common
format, and the ontological information extracted from their metadata is standardized. Aided by
this information, experts synthesize VQA pairs designed to test perception ability. Cognition
dataset (Right): First, domain experts use an interactive web application to upload their images and
corresponding open-ended VQA pairs. Next, GPT-4 transforms the VQA pairs into a close-ended
multiple-choice format. All GPT-4 generations are reviewed by experts before being incorporated
into the cognition dataset. </p>
              <div style="text-align: justify; text-justify: inter-word;">
        </div>

  </div>

</div>

</div>


</section>
{{ end }}

{{ define "sidebar-prefooter" }}
<!-- <section class="section section-sm">


  <div class="container">
    <div class="row justify-content-center text-center">
      <div class="col-lg-5">
        <h2 class="h4"><a href="docs/dataset/access">Download instructions</a></h2>
        <p>We will make the full dataset available to
          researchers under a standard research
          DUA in coming months. See <a href="docs/dataset/access">here</a> for more details.
        </p>
        <a class="btn btn-primary btn-lg px-4 mb-2" href="https://medalign-4b245550a5e9.herokuapp.com/"
          role="button">Get access</a>
      </div>
      <div class="col-lg-5">
        <h2 class="h4"><a href="https://medalign-4b245550a5e9.herokuapp.com/">Contribute instructions</a></h2>
        <p>Click <a href="https://medalign-4b245550a5e9.herokuapp.com/">here</a> to Share your email, contribute an instruction, and you'll be the first to know when the full dataset becomes
          available.</p>
      </div>
      <div class="col-lg-5">
        <h2 class="h4"><a href="https://github.com/som-shahlab/medalign/discussions">Contact us</a></h2>
        <p><a href="https://github.com/som-shahlab/medalign/discussions">Contact us</a> if you have any questions, feedback, or suggestions for MedAlign, or if you want to contribute!
        </p>
        <a class="btn btn-primary btn-lg px-4 mb-2" href="https://medalign-4b245550a5e9.herokuapp.com/"
          role="button">Team</a>
      </div>
    </div>
  </div>
</section> -->








<div class="container">
<div class="row justify-content-center">
  <h3>Citation</h3>
  <pre id="arxiv-citation">
@article{fleming2023medalign,
  title={MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records},
  author={Scott L. Fleming and Alejandro Lozano and William J. Haberkorn and Jenelle A. Jindal and Eduardo P. Reis and Rahul Thapa and Louis Blankemeier and Julian Z. Genkins and Ethan Steinberg and Ashwin Nayak and Birju S. Patel and Chia-Chun Chiang and Alison Callahan and Zepeng Huo and Sergios Gatidis and Scott J. Adams and Oluseyi Fayanju and Shreya J. Shah and Thomas Savage and Ethan Goh and Akshay S. Chaudhari and Nima Aghaeepour and Christopher Sharp and Michael A. Pfeffer and Percy Liang and Jonathan H. Chen and Keith E. Morse and Emma P. Brunskill and Jason A. Fries and Nigam H. Shah},
  journal={arXiv preprint arXiv:2308.14089},
  year={2023}
}
</pre>
</div>
</div>
{{ end }}

{{ define "sidebar-footer" }}
<!-- <section class="section section-sm container-fluid">
  <div class="row justify-content-center text-center">
    <div class="col-lg-9">
      {{- .Content -}}
    </div>
  </div>
</section> -->
{{ end }}
